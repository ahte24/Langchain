# ğŸ¦œğŸ”— LangChain RAG Pipeline â€” Pinecone + Google Gemini

A **Retrieval-Augmented Generation (RAG)** demo built with LangChain, Pinecone vector store, and Google Gemini. This project compares three approaches to answering questions â€” raw LLM inference, manual RAG chain, and a declarative LCEL-style RAG chain.

---

## âœ¨ Features

- ğŸ” **Semantic Search** â€” Retrieve relevant document chunks from Pinecone using vector similarity
- ğŸ¤– **Google Gemini** â€” Uses `gemini-2.5-flash` as the LLM and `gemini-embedding-001` for embeddings (free tier)
- ğŸ§± **Two RAG styles** â€” Manual step-by-step chain vs. declarative LCEL pipeline
- ğŸ“¥ **Document Ingestion** â€” Load, split, embed, and upsert any text file into Pinecone
- âš–ï¸ **Side-by-side comparison** â€” See how RAG improves over raw LLM responses

---

## ğŸ“ Project Structure

```
Langchain Vector Gist/
â”œâ”€â”€ main.py            # RAG query pipeline (3 comparison modes)
â”œâ”€â”€ ingestion.py       # Document loader + Pinecone upsert
â”œâ”€â”€ mediumblog1.txt    # Sample knowledge base document
â”œâ”€â”€ textsplitters.md   # Notes on LangChain text splitting strategies
â”œâ”€â”€ pyproject.toml     # Project metadata and dependencies (uv)
â”œâ”€â”€ uv.lock            # Locked dependency versions
â”œâ”€â”€ .python-version    # Python version pin
â”œâ”€â”€ .gitignore         # Standard Python gitignore
â””â”€â”€ README.md          # This file
```

---

## ğŸš€ Getting Started

### Prerequisites

- Python **3.13+**
- [`uv`](https://github.com/astral-sh/uv) package manager
- A [Pinecone](https://www.pinecone.io/) account and index
- A [Google AI Studio](https://aistudio.google.com/) API key

### 1. Clone the repository

```bash
git clone https://github.com/ahte24/Langchain.git
cd Langchain
```

### 2. Install dependencies

```bash
uv sync
```

### 3. Set up environment variables

Create a `.env` file in the root of the project:

```env
GOOGLE_API_KEY=your_google_api_key_here
INDEX_NAME=your_pinecone_index_name_here
PINECONE_API_KEY=your_pinecone_api_key_here
```

> âš ï¸ Never commit your `.env` file. It is already listed in `.gitignore`.

---

## ğŸ“¥ Ingesting Documents

Before querying, you need to load your documents into Pinecone. The `ingestion.py` script handles this:

```bash
uv run python ingestion.py
```

**What it does:**

1. Loads `mediumblog1.txt` using LangChain's `TextLoader`
2. Splits the text into **1000-character chunks** using `CharacterTextSplitter`
3. Generates embeddings via `GoogleGenerativeAIEmbeddings` (`gemini-embedding-001`)
4. Upserts all chunks into your Pinecone index

---

## ğŸ” Running the RAG Pipeline

```bash
uv run python main.py
```

The script runs three modes back-to-back for comparison:

| Mode                     | Description                                                      |
| ------------------------ | ---------------------------------------------------------------- |
| **0 â€” Raw LLM**          | Query sent directly to `gemini-2.5-flash` with no context        |
| **1 â€” RAG without LCEL** | Manual retrieval â†’ format â†’ prompt â†’ LLM chain                   |
| **2 â€” RAG with LCEL**    | Declarative pipe-style chain using LangChain Expression Language |

### Example Query

```python
query = "What is Pinecone in machine learning"
```

---

## ğŸ§  Architecture

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚           mediumblog1.txt         â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                     â”‚ ingestion.py
                                     â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚     Pinecone Vector Store         â”‚
                    â”‚  (gemini-embedding-001 vectors)   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                     â”‚
              User Query             â”‚  Similarity Search (k=3)
                  â”‚                  â–¼
                  â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â””â”€â”€â”€â”€â–ºâ”‚     Retriever           â”‚
                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                     â”‚ Retrieved Chunks
                                     â–¼
                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                         â”‚   ChatPromptTemplate   â”‚
                         â”‚  (context + question)  â”‚
                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                     â”‚
                                     â–¼
                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                         â”‚  gemini-2.5-flash LLM  â”‚
                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                     â”‚
                                     â–¼
                               Final Answer
```

---

## ğŸ“¦ Dependencies

| Package                  | Purpose                                |
| ------------------------ | -------------------------------------- |
| `langchain`              | Core LangChain framework               |
| `langchain-community`    | Community loaders (TextLoader, etc.)   |
| `langchain-google-genai` | Gemini LLM + Embeddings integration    |
| `langchain-pinecone`     | Pinecone vector store integration      |
| `langchain-ollama`       | Ollama local LLM support               |
| `langchainhub`           | Pull prompts from LangChain Hub        |
| `python-dotenv`          | Load environment variables from `.env` |
| `black`                  | Code formatter                         |
| `isort`                  | Import sorter                          |

---

## ğŸ”— References

- [LangChain Docs](https://python.langchain.com/docs/)
- [Pinecone Docs](https://docs.pinecone.io/)
- [Google AI Studio](https://aistudio.google.com/)
- [LangChain Expression Language (LCEL)](https://python.langchain.com/docs/expression_language/)

---

## ğŸ“„ License

This project is open source and available under the [MIT License](LICENSE).
